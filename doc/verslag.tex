\documentclass[a4paper,draft]{article} 
\usepackage[noheadfoot,margin=2cm,a4paper]{geometry} 
\usepackage{amsmath, amsthm}
% \addtolength{\textheight}{20mm}
% \addtolength{\voffset}{-5mm}
\setlength{\footskip}{1cm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{pgf}
%\usepackage{listings}
\usepackage{amsmath}

\graphicspath{{./images/}}


\title{Stereo Vision using the OpenCV library} \author{Sebastian
Dr\"oppelmann \\ Moos Hueting \\ Sander Latour \\ Martijn van der
Veen} \date{June 2010}
\begin{document}
%numbering for the equations and figures
\numberwithin{equation}{section}
\numberwithin{figure}{section}

\begin{titlepage}
  \maketitle
  \thispagestyle{empty}
\end{titlepage}

\tableofcontents 
\thispagestyle{empty}
\newpage

\section{Preface} 
Stereo vision is one of the key subjects in the computer vision
research. Stereo vision can be best described as taking two viewpoints
in a 3D world, comparing the distance between the position of an
object in both images and relating that to the distance of the object
to the camera. Such information is retrieved by a dense stereo
algorithm of which the output is often a disparity depth map. A
disparity depth map is a 2D image where the color of each pixel is
directly linked to the distance of the pixel on that coordinate in the
original image, in other words if an object is white it is depending
on the implementation nearer or further away than a darker object. Our
goal is to generate such a depth map from two images taken with two
webcams.

Depthmaps are interesting because they can be used for various
purposes:
\begin{description}
 \item[3D modeling of 2D images] When you take two 2D images of a 3D
environment and calculate the depthmap, you can create a 3D model of
the scene by using the depth as the third dimension.
 \item[Tracking of objects] When you have a depthmap it is easier to
track an object because you have additional segmentation
possibilities. You can create segments of pixels that are near based
on the depth of the pixels and their adjacency.
 \item[Recognizing front objects] When you apply segmentation based on
the depthmap, you can distinguish objects that are situated in the
front of the scene.
 \item[As information about the environment in path planning] A
depthmap supplies additional information for path planning.
\end{description}

\section{Practical problems}

\subsection{Webcams} 
We will have to make the two webcams work on Linux. Ideally we would
have a live feed from both webcams at all times.

\subsection{OpenCV} 
We will have to get acquainted with the library OpenCV. See section
\ref{opencv}.

\section{Theory}

\subsection{Camera calibration}
When working with stereo vision we need to know the spatial relation
between the two cameras, and we need to get rid of radial distortion due to
the imperfections of the lens. Using the algorithm proposed by Zhang
\cite{zhang1999} we can automate this process by showing both cameras
different orientations of a chessboard. A simple contrastbased algorithm
can recognize the black-white intersections of the chessboard squares. The
chessboard used can be of any size N by M. This way for each chessboard we
show we have $N*M$ points we know for both cameras. Using the disparities of
these points between both cameras, we can calculate a rotation/translation
matrix which transforms the coordinate system of the left camera to the
coordinate system of the right camera, or the other way around.

We have chosen not to describe the algorithm fully here. See the article by
Zhang for more information \cite{zhang1999}.

\subsection{Epipolar geometry and rectification}
\label{recttheory}
\emph{Epipolar geometry} is used in stereo vision to limit the searching space
when looking for matching points in both images. A point $X$ in 3D space is seen
in image $A$, which we will call the source image, as a point $x$, which is on
the line between camera $A$'s focal point and point $X$. This line is seen on
image $B$, which we will call the search image, as a line. This is called an
\emph{epipolar line}. Given both the cameras internal and external matrices and
a point $x_A$ we can generate an epipolar line corresponding to this point in
the search image. This constrains the search space to this 1D line. However,
this means that for each pixel in the source image, we have to calculate the
corresponding epipolar line in the search image. It would be much more
convenient if each epipolar line was on the same line as the pixel it
corresponded to. It is possible to transform the images in such a way that the
epipolar lines are parallel and horizontal, and the process is called
rectification.

Figure \ref{fig:rectepipole} demonstrates the process of rectification.  The
points called $E_{1}$ and $E_{2}$ are called the \emph{epipolar points} of both
images. All epipolar lines intersect the epipolar point of a given image.
However, when the retinal planes of both cameras lie on the same plane, the
epipolar points move to infinity and the epipolar lines in one image become
parallel to one another. The line between the optical centers of both retinal
planes is called the baseline. When the epipolar points lie at infinity, we can
see that the epipolar lines are parallel to this baseline. That means that if
the baseline is parallel to the X-axis, the epipolar lines are horizontal.

\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{nonrectepipole}
$\rightarrow$
\includegraphics[width=0.4\textwidth]{rectepipole}
\caption{Left: unrectified cameras, right: rectified cameras. After
rectification, the epipolar lines are colinear and horizontal}
\label{fig:rectepipole} \end{figure}

\subsubsection{Algorithm}
We will give the basic gist of the rectification algorithm here. 

\begin{enumerate}
  \item First both camera matrices are separately factorized into three parts:
  \begin{itemize}
    \item The internal camera matrix $A_o$
    \item The rotational matrix $R_o$ that gives the rotation between the camera's
    frame of reference and the world frame of reference
    \item The translation matrix $t_o$ that gives the translation between the
    camera's frame of reference and the world frame of reference
  \end{itemize}
  This means $P_o = A_o[R_o\ |\ t_o]$
  \item The new rotational matrix $R_n$ is constructed. This matrix makes
  sure that in the new `cameras' reference systems, the x-axis is parallel
  to the baseline. The baseline is simply the line between the two optical
  centers, which can be retrieved from $P_o$.
  \item The new internal camera matrix $A$ is constructed. This can be
  chosen arbitrarily and in this algorithm the mean between both original
  inner camera matrices ($A_{o1}, A{o2}$) is used.
  \item The new camera matrices are now $P_{n1} = A[R\ |\ -R c_1]$ and
  $P_{n2}
  = A[R\ |\ -R c_2]$. The optical centers are unchanged.
  \item A mapping is created that maps the original image planes of $P_{o1},
  P_{o2}$ to the new $P_{n1}, P_{n2}$. Because in general the pixels in the new
  images don't correspond to integer positions in the old images, bilinear
  interpolation is used to fill up the gaps.
\end{enumerate}

For a complete and in depth description of each step, please see
\cite{fusiello2000}.

\subsection{Dense Stereo} 
Dense stereo combines the two images you get from the rectification
and calculates the position of the pixels in the left image and
outputs where the pixel is located in the right image. With this
method we calculate the pixel's distance from the camera.  The depth
is then translated to a depth map where points closer to the camera
are almost white whereas points further away are almost black. Points
in between are shown in gray-scale, which get darker the further away
the point gets from the camera. See Figure \ref{dm_example} for an
example depth map with original image.

\begin{figure} \centering
\includegraphics[width=0.3\textwidth]{depthmap}
\includegraphics[width=0.3\textwidth]{depthmap_original}
\caption{A depthmap with the corresponding picture, gray values show
  the depth of the image}
\label{dm_example}
\end{figure}

To achieve this you have a whole list of algorithms that do the
trick.\footnote{A good overview can be found at
http://vision.middlebury.edu/stereo/eval/}\\
Most of these algorithms are based on 4 principles:
\begin{itemize}
\item Graph Cut
\item Believe Propagation
\item Region Based / Block Matching
\item Dynamic Programming
\end{itemize}
These algorithms have to deal with the following problems in their
calculations
\begin{itemize}
    \item Matching points in both images
    \item Occlusion
\end{itemize}


\subsubsection{Matching}
\begin{figure} [h!tb]
\centering
\includegraphics[width=0.8\textwidth]{matching_problems_direction}
\caption{On the left image the window is completely on the left of the
  rightmost pole, whereas in the right image, the window is completely
  on the right side of the rightmost pole}
\label{dirprob}
\end{figure}

The main goal of such an algorithm is of matching one point in one
image to the corresponding point in the other image. During the
matching there are several tasks that the algorithm has to perform. At
first it has to compare the epipolar lines of the images pixel by
pixel. For every pixel on one line you have to find the counterpart on
the corresponding epipolar line in the other image. Often the pixels
aren't in the same order, for example if there is a lamp pole in front
of a house, things that lie on the left side of the lamp pole in the
left picture could sit on the right side in the right picture. See
Figure \ref{dirprob}.

\begin{figure} [h!tb]
\centering
\includegraphics[width=0.6\textwidth]{matching_problems_occlusion}
\caption{Two examples of occlusion problems when working with stereo
vision}
\label{occprob} 
\end{figure}

Another problem is occlusion of the pixels. Some things that are
visible in one picture can be hidden behind objects in the other
picture. This has to be caught and handled. Both examples in Figure
\ref{occprob} show that the left point of the left eye is not visible
from the right eye's view.

Because the OpenCV library that we have chosen has the basic
implementation of a graph cut algorithm from
Kolmogorov\cite{kolmogorov2003}, we will start with that specific
algorithm. It is a bit slow and not the best algorithm to handle
occlusion, but because it is easy to use we will at first use this
algorithm and later replace it with an implementation of the block
matching algorithms also provided by OpenCV.

\subsubsection{Graph Cut Theory}
\label{gc_theory}
Normal stereo matching algorithms try to match a pixel in the left
image to a pixel in the right image based on some individual property
like color. Although this is a fast and reasonably accurate process it
does not deal with the interlinear consistency.  The interlinear
consistency forces you to also look at interpixel properties like
neighbours and will result in a much more accurate desparity map.  \\
In order to maintain the interlinear consistency you want to connect
the pixels that are adjacent within a line and add some sort of weight
on that connection to make it expensive to break that
connection. Furthermore you want to determine the disparities in the
entire picture that disrupt those neighbouring relations as less as
possible. [ref] states that given some energy function you can
construct a graph where the labeling with the lowest energy is equal
to the minimum cut on that graph. In that respect you can look at the
stereo correspondance problem as a labeling problem where you have to
assign disparity labels.  We will explain this in various sections
below.\\\\
\begin{figure}[t] \centering
\includegraphics[width=180px]{Network_flow.png}
\caption{flow network}
\label{flow_network}
\end{figure}
\noindent\textbf{Graph theory: flow networks}:\indent\\
Figure \ref{flow_network} shows a directed graph $\mathcal{G}$ which
can be formaly defined as $\mathcal{G} = \{\mathcal{V},\mathcal{E}\}$.
$\mathcal{V}$ depicts the set of vertices or nodes in the graph,
whereas $\mathcal{E}$ depicts the set
of edges between vertices.\\\\
\noindent\textbf{Graph theory applied to stereo vision}:\indent\\
\begin{figure}[t!bt]
\begin{enumerate}
 \item Create a graph $\mathcal{G} = \{\mathcal{V},\mathcal{E}\}$
 \item $\forall_{p \epsilon \mathcal{P}}$ add $p$ to $\mathcal{V}$
 \item add terminals $f^0$ and $f^a$ to $\mathcal{G}$
 \item $\forall_{p,q \epsilon \mathcal{N}}$ add \{$p$,$q$\} to $\mathcal{E}$
 \item $\forall_{p \epsilon \mathcal{V}}$ add $\{f^0,p\}$ and $\{f^a,p\}$ to $\mathcal{E}$
 \item $\forall_{\{p,q\} \epsilon \mathcal{E}}$ set weight $w$ for $\{p,q\}$
\end{enumerate}
\caption{Steps for constructing a graph from picture $\mathcal{P}$}
\label{alg:gc_construct}
\end{figure}[h!bt]
Figure \ref{alg:gc_construct} describes the process of constructing a
graph $\mathcal{G}$ as discussed in the previous section from a
picture $\mathcal{P}$. $\mathcal{N}$ depicts a set of horizontally
adjacent pixel pairs.\\\\
\noindent\textbf{Weights}:\indent\\
\begin{equation}
 E(f) = E_{data}(f) + E_{smooth}(f) + E_{occ}(f)
\end{equation}
When two adjacent pixels have similair intensities it is likely that
they are part of the same object, this means that during the labeling
of disparities you would \emph{prefer} to assign the same label to
both pixels.  The weights in $\mathcal{G}$ encode this information,
the higher the weight value the less you want to break the pixels
apart.
\begin{figure}[h!bt]
\centering
\caption{Picture $\mathcal{P}$}
\begin{minipage}[h]{.4\linewidth}
\include{pgf1}
\end{minipage}
\hspace{.1\linewidth}
\begin{minipage}[h]{.4\linewidth}
\include{pgf2}
\end{minipage}
\end{figure}
\begin{equation}
 f_{p}^{C} = \left\{
  \begin{array}{lll}
  \alpha & \text{if} & t^{\alpha}_{p} \epsilon \mathcal{C} \\ 
  f_{p}  & \text{if} & t^{\alpha}_{p} \epsilon \mathcal{C}
  \end{array}
\right.
 \forall p \epsilon \mathcal{P}
\end{equation}

\begin{figure}[h!bt]
\centering
\include{pgf6}
\end{figure}

\begin{figure}[h!bt]
\centering
\include{pgf3}
\include{pgf4}
\caption{Graph-Cut step 1 with $f^\alpha = 1$}
\include{pgf5}
\include{pgf2}
\caption{Graph-Cut step 2 with $f^\alpha = 2$}
\end{figure}

\subsubsection{Semi Global Block Matching Theory}
Semi global block matching is build on the idea that you use mean data
from blocks of pixels as pixel energy and then build up a cost array
over the costs of all disparities from the minimum disparity to the
maximum disparity for all the pixels. Afterwards a search is done for
the minimum path cost in several directions of the image. The minimum
path cost is calculated from the pixel energy with penalties for the
disparity difference to neighbour pixels along a path that ends in the
pixel the disparity is searched for, where the one path with the
lowest cost is the right one. This is done on
subpixel level. After calculation of the disparities there are some
refinement steps to filter out peaks and noise.\\
The Middlebury page divides disparity algorithms mainly into four
parts, where some of the parts are optional. These parts are:
\begin{itemize}
\item cost computation
\item cost aggregation
\item disparity computation/optimization
\item disparity refinement
\end{itemize}
\noindent For the SGBM algorithm these steps are filled in as follows:\\
\textbf{Cost computation} can be done from the intensity or color
pixel values. Radiometric differences also have to be kept in mind, so
the gradient can also be calculated.\\
\textbf{Cost aggregation} connects the costs of the neighbourhood of
pixels to find the cheapest (thus matching) pixel in the compare
image. This is done through a global energy function from all
directions through the image.\\
\textbf{Disparity computation} calulates the real disparity from the
previously calculated energy through a winner-takes-all
implementation.\\
\textbf{Disparity refinements} are used to further stabilize the
disparity map. This is done via peak filtering, intensity consistent
disparity selection and gap interpolation. Also multibaseline matching
is done through the fusion of disparities to circumvent streaking and
add consistency. Because refinement is a separate part and has nothing
to do with stereovision per se, we leave it out in the explanation\\

\emph{\textbf{Cost Calculation}}:\\
% the costs of this algorithm is caluclated from the intensity of the
% base pixel $I_{m\mathbf{p}}$ and the match pixel intensity
% $I_{m\mathbf{q}}$. Here the formula $\mathbf{q} = e_{bm}(\mathbf{p},
% d) = [p_x - d, p_y]^T$ is used where $d$ represents the disparity in
% rectified images. The size of the matching area can influence
% robustness of the images. Larger areas are more robust but fine
% structures are blurred more, because of the assumption of the same
% disparity over the whole area which isn't always true.
The \emph{matching cost} for the blocks of pixels used for calculating
the disparity in the OpenCV implementation are calculated through the
implementation of the subpixel algorithm described in the paper of
Birchfeld and Tomasi: "Depth Discontinuities by Pixel-to-Pixel Stereo"
\cite{birchtom99}. The size of the area has influence on the
robustness of the disparity map. Larger areas are more robust but fine
structures get more blurred, because of the assumption of the same
disparity over the whole area which isn't always true. The disparity
cost $C_{BT}( \mathbf{p},d)$ is calculated for a chosen squareblock of
pixels $\mathbf{p}$ from $minX$ till $maxX$, the limits of the block,
where $d$ represents the current disparity in the rectified images and
$x$ is the pixel in the current block. \eqref{eq:costcalc} This is
done over all disparities between the given minimum and maximum
disparity $minD$ and $maxD$.\\
\begin{equation}
  \label{eq:costcalc}
  C_{BT}[(x-minX)*maxD - minD + (d - minD)]
\end{equation}
% For each pixel $row1[x], max(-maxD, 0) \leq minX \leq x < maxX \leq
% width - max(0, -minD)$, and for each disparity $minD \leq d < maxD$
% the function computes the cost $(C[(x-minX)*(maxD - minD) + (d -
% minD)])$, depending on the difference between $row1[x]$ and
% $row2[x-d]$.\\

\emph{\textbf{Cost Aggregation}}:\\ 
To smoothen wrong disparities calculated by the cost function and
finding the right ones, the Energy of the disparity image is
calculated from the sum of all pixel matching costs for the disparity
of $D$. Different penalties, $P_1, P_2$ are applied for small and
large disparity changes \eqref{eq:enerform}.
\begin{equation}
E(D) = \displaystyle\sum\limits_{\mathbf{p}}
(C_{BT}(\mathbf{p},D_\mathbf{p}) +
\displaystyle\sum\limits_{\mathbf{q} \in N_\mathbf{p}} P_1
T[|D_\mathbf{p} - D_\mathbf{q}| = 1] +
\displaystyle\sum\limits_{\mathbf{q} \in N_\mathbf{p}} P_2
T[|D_\mathbf{p} - D_\mathbf{q}| > 1]) 
%  \caption{Energy Formula}
  \label{eq:enerform}
\end{equation}
The 
penalty for small changes in disparity $P_1$ is a constant
whereas the penality for higher disparity changes $P_2$ is also used
to catch discontinuities on intensity changes. This can be done
through the use of the intensity of neighbouring pixels $\mathbf{p}$
and $\mathbf{q}$ in the base image $I_b$\eqref{eq:penalty}.
But it always has to be ensured that $P_1 \leq P_2$. Now matching
is '\textit{only}' a question of minimizing the Energy $E(D)$.
\begin{equation}
  \displaystyle\frac{P_{2}^{'}}{|I_{b\mathbf{p}}-I_{b\mathbf{q}}|}
\label{eq:penalty}
\end{equation}
%\emph{\textbf{Energy minimization}}:\\ 
Because 2D energy minimization would be a NP-complete problem, 1D
Computation is used which can be calculated in polynomial time. To
cover the equally important information of different directions of the
image, 1D lines from '\textit{all}' directions which end in the
pixelblock are considered.\ref{fig:directions}
\begin{figure}
  \centering
  \includegraphics[width=0.3\textwidth]{directions}
  \caption{Directions of paths considered}
  \label{fig:directions}
\end{figure}
\noindent In the Hirschm\"{u}ller paper 8-16 paths
from all directions are recommended, whereas the standard algorithm in
OpenCV calculates 5 or 8 directions due to the high memory cost. For
our implementation we used 8 directions. For every path chosen the
smoothed path cost $S(\mathbf{p},d)$ is calculated through the
traversing path cost $L_{r}(\mathbf{p},d)$, were the cost represents
the energy formula \eqref{eq:enerform} along \textbf{an} arbitrary 1D
path \eqref{eq:trav}. Because the numbers can add up this way to
really huge numbers, the minimum path cost of the previous pixel $\mathit{k}$ is
substracted. This way the numbers stay smaller and the path doesn't
change it's minimum cost way, because the minimum cost of the pixel
before is a constant.  For precalculation all costs can be saved in an
integer array of size $[W*H*D]$ and the aggregated costs are then
saved in an equally sized array $S$.
\begin{align}
  \label{eq:trav}
  L_{r}(\mathbf{p},d) = 
  C_{BT}(\mathbf{p},d) + min(L_{r}(\mathbf{p} -
  \mathbf{r}, d), 
  \notag\\
  L_{r}(\mathbf{p} - \mathbf{r}, d - 1) + P_1,
  \notag\\
  L_{r}(\mathbf{p} - \mathbf{r}, d + 1) + P_1,
  \\
  \displaystyle\min\limits_{\mathit{i}} L_{r}(\mathbf{p} - \mathbf{r}, \mathit{i}) +
  P_2) -
  \notag\\
  \displaystyle\min\limits_{\mathit{k}} L_{r}(\mathbf{p} -
  \mathbf{r}, \mathit{k})\notag
\end{align}

\emph{\textbf{Disparity Computation}}:\\
The base disparity map $D_b$ from the base image $I_b$ is caluclated by
picking the disparity with the minimum path cost for each pixel. This
is done with subpixel accuracy by picking the minimum of a quadratic
curve through the neighbouring pixel costs. The disparity map $D_m$ of
the match image from $I_m$ is calculated by traversing the epipolar
line of pixel $q$ in the match image. The disparity is then again the
disparity of the lowest pathcost. To enhance the quality of the
disparity map, you can calculate the disparities again but with $I_m$
as base image and $I_b$ as match image. After $D_m$ and $D_b$ have
been calculated, a consitency check is done between the two. If the
consistency between the two is too great (e.g. due to occlusion) the
disparity is set to invalid. Also unique constrained can be switched
on, which enforces one on one pixel mappings.

\section{Implementation}

\subsection{OpenCV}
\label{opencv} OpenCV is a library of programming functions for real
time computer vision. By ussing this library we can constrain our tasks
to integrating various parts of OpenCV and expanding it where
possible. If we would not use OpenCV, we would not have enough time to
achieve our goal. OpenCV is a C library but has python binding which
we will use to decrease the risk of programming errors.

\subsection{Calibration}
\label{calib_implement} OpenCV has a separate function for calibrating
a set of stereo cameras. This function\footnote{See stereoCalibrate in
the OpenCV documentation} uses as input a list of coordinates of
points in the left image and the coordinates of the same points in the
world in the right image, and a set of coordinates where these points
actually lie in the real world (3D coordinates).

\subsubsection{Chessboard points} As input for the calibration, we use
the intersection points of a chessboard. These points on the
chessboard are recognized by the stereo cameras and a list of
coordinates is returned, see Figure \ref{chessboardcorners}. The
algorithm for this recognition isn't covered here.\footnote{See the
OpenCV documentation for the function findChessboardCorners for more
information}

Because we are looking for the relationship between the cameras and
not the relationship between the cameras and the actual world, we can
choose the origin for these ``real world points'' however we like. As
we are using the chessboard, it is very convenient to choose the x and
y axes along the sides of the chessboard, so that the first
intersection lies at $(0, 0, 0)$ in the real world.

\begin{figure} [h!tb]
\centering
  \includegraphics[width=0.3\textwidth]{chessboardcorners}
  \caption{The recognized chessboard-points by the OpenCV function
`findChessboardCorners'\label{chessboardcorners}}
\end{figure}

\subsubsection{Output} The calibration function outputs a set of
camera matrices, a set of distortion coefficients for both cameras (to
correct for lens distortion) and a translation/rotation matrix
relating the first camera to the second. This output is used by the
rectification algorithm explained below.


\begin{figure}[h] \centering
  \includegraphics[width=0.4\textwidth]{leftown}
  \includegraphics[width=0.4\textwidth]{rightown}
  \caption{Unrectified images}
  \label{unrectified}

  \includegraphics[width=0.4\textwidth]{leftownr}
  \includegraphics[width=0.4\textwidth]{rightownr}
  \caption{Rectified images}
  \label{rectified}
\end{figure} \newpage

\subsection{Rectification}
The algorithm used for rectification in OpenCV is exactly as we have
explained in the theory section (\ref{recttheory}).

\subsection{Dense Stereo Algorithms}
During our project we first started with using the standard algorithms
that are already implemented in OpenCV. These algorithms are namely:
\begin{itemize}
\item Graph Cut
\item Block Matching
\item Semi Global Block Matching
\end{itemize}
To test these algorithms we used the dataset from Middlebury
\cite{middlebury}, which is a good set with rectified images and their
belonging depth map, calculated with Graph Cut.

\subsubsection{Graph Cut}
The graph cut algorithm is one of the most popular algorithms in the
stereo vision depth map generation. It is quite slow though and new
algorithms return better results qua speed and depth recognition. We
used the standard implementation in OpenCV which is written by use of
the Kolmogorov paper from 2003\cite{kolmogorov2003}. Our first results
almost reach the quality of the depth map provided by the Middlebury
homepage\cite{middlebury}. See Figure \ref{gc_comp}. In the final
version we will explain how it works in the theory part. %\ref{gc_theory}

\begin{figure} [h!tb]
  \centering
  \includegraphics[width=0.4\textwidth]{gc_tsukuba_own}
  \includegraphics[width=0.4\textwidth]{disp_tsukuba_orig}
  \caption{left: Our GC depthmap, right: Middlebury Ground Truth}
  \label{gc_comp}
\end{figure}

\subsubsection{Block Matching}
The Block Matching algorithm is much faster than the Graph Cut
Algorithm, but the quality of the results till now are not as good.
There are more possibilities to fine-tune the algorithm with pre and
post filters which have huge impact on the quality and we still have
to find the right configuration to get the best results. See Figure
\ref{bm_comp} for comparison with the Graph Cut algorithm from
Middlebury. We couldn't find any good block matching example to
compare with but with the graph cut you will get the idea of how it
has to divide the image into different depths.

\begin{figure} [h!tb]
  \centering
  \includegraphics[width=0.4\textwidth]{bm_tsukuba_own}
  \includegraphics[width=0.4\textwidth]{disp_tsukuba_orig}
  \caption{left: Our BM depthmap, right: Middlebury Ground Truth}
  \label{bm_comp}
\end{figure}

\subsubsection{Semi Global Block Matching}
The Semi Global Block Matching algorithm is quite new in the OpenCV
library. It was implemented in version 2.1 which is the current
version at the time this article is written. It is more precise and
faster than the standard block matching algorithm but the python
binding is not yet completed and integrated into OpenCV. Because of
this we wrote this part in C++. Just as the standard Block Matching
the algorithm still needs to be tuned right to return the optimal
depthmap. See Figure \ref{sgbm_comp} for comparison with the GC
algorithm. The tuning of the algorithm was one of the most important
steps to get good results. The options you could tune were: 
\begin{itemize}
\item min and max disparity
\item SAD size
\item PreFilterCap
\item disp12MaxDiff
\item uniqueness
\item speckleWindowSize and speckleRange
\item Original
\end{itemize}
\noindent\emph{min and max disparity} are the minimum and maximum disparity
values that can be found. Between these the best one is chosen for the
disparity image. In our implementation we set the minimum disparity to
0 and the maximum disparity we chose the image width/8, because it
returned the best results\\
\emph{SAD size} is the size of the blocks which is considered for the
Energy calculation of the algorithm. The manual suggests values
between 5 and 21 pixels. The value has to be an uneven value. We chose
for a size of 9 to keep the calculation faster and we didn't get much
better results with higher values.\\
\textbf{PreFilterCap} is the value for the Tomasi cost function to cap
the values at [-PreFilterCap, PreFilterCap] intervals. In the OpenCV
implementation the values given can vary between 0 and 63. We chose
for 63 because then you had the minimum noise inside the image.\\
\textbf{disp12MaxDiff} is the maximum value in the left-right
disparity check. We set it to 2 to get results that where
acceptable. More didn't make a difference and less gave much noise.\\
\textbf{uniqueness} is the switch to enable the uniqueness check. We
switched it on.\\
\textbf{speckleWindowSize} defines the maximum region which is
considered as speckle / peak. In our configuration we set the value to
100. A value less gave too much noise and a higher value didn't really
change that much. OpenCV suggests a value between 50-200.\\
\textbf{speckleRange} defines the disparity difference that is
considered as speckle. This has to be a value devidable by 16 and the
OpenCV manual suggests 16 or 32. With a value of 32 we had good
results so we kept that.\\
\textbf{Original} at last defines if you want to use 5 or 8 paths for
the cost aggregation. We used the original suggested 8 paths so we
turned on the boolean.

\begin{figure} [h!tb]
  \centering
  \includegraphics[width=0.4\textwidth]{sgbm_tsukuba_own}
  \includegraphics[width=0.4\textwidth]{disp_tsukuba_orig}
  \caption{left: Our SGBM depthmap, right: Middlebury Ground Truth}
  \label{sgbm_comp}
\end{figure}


\section{Planning}
\begin{itemize}
  \item Week 1
    \begin{itemize}
      \item Reading literature
      \item Getting webcams to work
      \item Choosing dense algorithm
    \end{itemize}
  \item Week 2
    \begin{itemize}
      \item Implementing
        \begin{itemize}
          \item Dense disparity map algorithm working
          \item Camera calibration using epipolar geometry
          \item Rectification of images
        \end{itemize}
      \item Halfway report
    \end{itemize}
    
  \item Week 3
    \begin{itemize}
    \item Fine tuning camera calibration
    \item Cropping of rectified images
    \item Fine tuning parameters of dense stereo
    \item Completely understand the algorithms
    \item Depth map normalize
    \end{itemize}

  \item Week 4
    \begin{itemize}
      \item Optimizing and testing
      \item If there's enough time left
        \begin{itemize}
          \item Generate 3D image of environment
          \item Remove background using dense disparity map
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Tasks}
  \begin{itemize}
    \item Martijn and Moos
    \begin{itemize}
      \item Camera calibration
      \item Epipolar geometry
    \end{itemize}
    \item Sander and Sebastian
    \begin{itemize}
      \item Finding corresponding points
      \item Generating depth map
    \end{itemize}
  \end{itemize}

\bibliographystyle{plain} \bibliography{verslag}
\end{document}
